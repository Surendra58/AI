# -*- coding: utf-8 -*-
"""ProtoNets-Omniglot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1We4qwJBBA8BwLuOIQZKLLU4MzskWDHUH

# Prototypical Networks
### on the Omniglot Dataset

## Setup
"""

https://github.com/theneuralbeing/prototypical_networks

Prototypical Networks  - AI-Metric Based Meta Learning(AGI)

# Clone the repository
!git clone https://github.com/theneuralbeing/prototypical_networks

cd prototypical_networks/

# Download
!wget https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip
!wget https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip
# Unzip to omniglot/ folder
!unzip images_background.zip
!unzip images_evaluation.zip

"""## Imports"""

import numpy as np
import os
import cv2
from scipy import ndimage
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt

import torch
from torch import nn
import torch.nn.functional as F
import torchvision

"""## Load Dataset"""

def load_characters(root, alphabet):
    '''
    Loads the characters from a given alphabet
    Args:
        root (str): Root directory
        alphabet (str): folder name of alphabet
    Returns:
        (tuple) of:
            (list): images
            (list): labels
    '''
    X = []
    y = []
    
    alphabet_path = os.path.join(root, alphabet)
    characters = os.listdir(alphabet_path)
    
    for char in characters:
        char_path = os.path.join(alphabet_path, char)
        images = os.listdir(char_path)
        
        for img in images:
            image = cv2.imread(os.path.join(char_path, img))
            image = cv2.resize(image, (28, 28)) / 255
            
            # Augmentations to add 3 new classes by rotating each class
            rotated_90 = ndimage.rotate(image, 90)
            rotated_180 = ndimage.rotate(image, 180)
            rotated_270 = ndimage.rotate(image, 270)
            
            X.extend((image, rotated_90, rotated_180, rotated_270))
            y.extend((f'{alphabet}_{char}_0', 
                      f'{alphabet}_{char}_90', 
                      f'{alphabet}_{char}_180', 
                      f'{alphabet}_{char}_270'))
    
    return X, y

def load_data(root):
    '''
    Loads the full omniglot dataset from a root directory
    Args:
        root (str): path of omniglot dataset
    Returns:
        (tuple) of:
            (ndarray): images
            (ndarray): labels
    '''
    X_data = []
    y_data = []
    
    alphabets = os.listdir(root)
    for alphabet in tqdm(alphabets):
        X, y = load_characters(root, alphabet)
        X_data.extend(X)
        y_data.extend(y)
    
    return np.array(X_data), np.array(y_data)

X_train, y_train = load_data('images_background/')

X_eval, y_eval = load_data('images_evaluation/')

"""## Task Generation"""

def extract_sample(X_data, y_data, k_shot=5, n_way=8, n_query=5, permute=True):
    '''
    Extract a random sample as a k-shot n-way task
    Args:
        X_data (ndarray): images
        y_data (ndarray): labels
        k_shot (int): number of images per class
        n_way (int): number of classes in the task
        n_query (int): number of query images
        permute (bool): flag to shuffle the classes or not. For training this must be set to True.
    Returns:
        (ndarray): sampled images of shape (n_way, k_shot + n_query, 28, 28, 3)
    '''
    sample = []
    
    # Randomly select n_way classes
    sampled_cls = np.random.choice(np.unique(y_data), n_way, replace=False)
    
    # Only useful for extracting samples with no query images during prediction on unlablled data
    if not permute:
        sampled_cls = np.unique(y_data)[:n_way]
        
    
    for c in sampled_cls:
        # Select images belonging to that class
        X_data_c = X_data[y_data == c]
        
        # Sample k_shot+n_query images
        sample_images = np.random.permutation(X_data_c)[:(k_shot+n_query)]
        sample.append(sample_images)

    sample = np.array(sample)
    # Convert to tensor and permute the images as channels first
    sample = torch.from_numpy(sample).float().permute(0, 1, 4, 2, 3)
    
    return sample

def display_sample(sample):
    '''Display the samples of the task as a grid'''
    # Reshape to 4-dimensions from 5-dimensions
    sample_4d = sample.view(sample.shape[0]*sample.shape[1], *sample.shape[2:])
    
    # Make a grid
    out = torchvision.utils.make_grid(sample_4d, nrow=sample.shape[1])
    
    plt.figure(figsize = (16,7))
    plt.imshow(out.permute(1, 2, 0))

test_sample = extract_sample(X_train, y_train, k_shot=5, n_way=8, n_query=5)

display_sample(test_sample)

"""## Model"""

class Image2Vector(nn.Module):
    def __init__(self):
        super(Image2Vector, self).__init__()
        
        self.input_block = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1),
                                         nn.BatchNorm2d(64),
                                         nn.ReLU(),
                                         nn.MaxPool2d(2))
        
        self.conv_block = nn.Sequential(nn.Conv2d(64, 64, 3, padding=1),
                                        nn.BatchNorm2d(64),
                                        nn.ReLU(),
                                        nn.MaxPool2d(2))
    def forward(self, x):
        
        x = self.input_block(x)
        x = self.conv_block(x)
        x = self.conv_block(x)
        x = self.conv_block(x)
        
        # Output of shape (batch_size, 64)
        out = x.view(x.size(0), -1)
        
        return out

class ProtoNet(nn.Module):
    '''ProtoNet Model for usage with known/labelled data'''
    def __init__(self):
        super(ProtoNet, self).__init__()
        
        self.encoder = Image2Vector()
    
    def forward(self, sample, k_shot, n_way, n_query, device):
        '''Forward pass
        Args:
            sample (torch.Tensor): sample of shape (n_way, k_shot + n_query, 28, 28, 3)
            k_shot (int): number of images per class
            n_way (int): number of classes
            n_query (int): number of query images per class
        
        Returns:
            tuple of:
                loss, dict of loss value, accuracy, predictions
        '''
        # Get the support set and query set images
        support = sample[:, :k_shot]
        queries = sample[:, k_shot:]
        
        # Get the target indices
        targets = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, n_query, 1).long().to(device)
        
        # Concatenate support set and query set along the 0th dimension
        x = torch.cat([support.contiguous().view(n_way * k_shot, *support.size()[2:]), 
                       queries.contiguous().view(n_way * n_query, *queries.size()[2:])], 0)
        
        # Get the embeddings
        embeds = self.encoder(x)
        z_dim = embeds.size(-1)
        
        # Compute Prototypes
        protos = embeds[:n_way * k_shot].view(n_way, k_shot, z_dim).mean(1)
        
        # Get query embeddings
        query_embeds = embeds[n_way * k_shot:]
        
        # Compute Distances
        distances = self.euclidean(query_embeds, protos)
        
        # Compute Log Softmax over the inverse of euclidean distances
        log_p = F.log_softmax(-distances, dim=1).view(n_way, n_query, -1)
        
        # Compute the negative of log softmax for correct classes (NLL)
        # and calculate the mean NLL loss
        loss = -log_p.gather(2, targets).squeeze().view(-1).mean()
                
        # Get predictions and calculate accuracy
        y_hat = log_p.argmax(2)
        accuracy = torch.eq(y_hat, targets.squeeze()).float().mean()
        
        return loss, {'loss': loss.item(), 
                      'preds': y_hat, 
                      'acc': accuracy.item()}
        
        
    def euclidean(self, queries, protos):
        '''
        Calculates Euclidean Distances between queries and prototypes
        Args:
            queries (torch.Tensor): shape (n, d) where n is n_way*n_query
            protos (torch.Tensor): shape (m, d) where m is n_way
        Returns:
            (torch.Tensor): shape (n, m) i.e. for each query the euclidean distances
                            to each of the prototypes
        '''
        n = queries.size(0)
        m = protos.size(0)
        d = queries.size(1)
        
        # Reshaping the queries and prototypes for distance calculation
        queries = queries.unsqueeze(1).expand(n, m, d)
        protos = protos.unsqueeze(0).expand(n, m, d)
        
        distance = torch.pow(queries - protos, 2).sum(2) # of shape n x m
        
        return distance

model = ProtoNet()

"""## Training"""

from tqdm.notebook import tnrange

def train(model, X_data, y_data, optimizer, device='cuda', 
          k_shot=5, n_way=60, n_query=5, epochs=5, episodes=2000):
    '''
    Trains the ProtoNet on k-shot n-way task
    Args:
        model
        X_data (torch.Tensor): images
        y_data (torch.Tensor): labels
        optimizer
        k_shot (int): number of support images per class
        n_way (int): number of classes
        n_query (int): number of query images per class
        epochs (int): maximum epochs
        episodes (int): number of steps per epoch
    '''
    
    model.to(device)
    model.train()
    # Create LR scheduler
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5)
    
    for epoch in range(epochs):
        print('Epoch {:d} Started...'.format(epoch+1))
        train_losses = 0
        train_accuracies = 0
        
        for episode in tnrange(episodes, desc='Epoch {:d}'.format(epoch+1)):
            optimizer.zero_grad()
            
            # Extract sample
            sample = extract_sample(X_data, y_data, k_shot, n_way, n_query).to(device)
            
            # Forward Pass on ProtoNet
            loss, stats = model(sample, k_shot, n_way, n_query, device)
            
            # Backprop and Optimize
            loss.backward()
            optimizer.step()
            
            # Get Stats
            train_losses += stats['loss']
            train_accuracies += stats['acc']
            
            if episode % 200 == 0:
                print('At Episode {:d} -- Loss {:.5f} - Acc: {:.3f}'.format(episode, stats['loss'], stats['acc']))
        
        # Print Epoch Stats
        mean_loss = train_losses / episodes
        mean_acc = train_accuracies / episodes
        print('Epoch {:d} -- Loss: {:.5f} - Acc: {:.3f}\n'.format(epoch+1, mean_loss, mean_acc))
        
        # Scheduler Step
        scheduler.step()

optimizer = torch.optim.SGD(model.parameters(), lr=0.001)

train(model, X_train, y_train, optimizer, device='cuda', k_shot=5, n_way=60, n_query=5, epochs=8, episodes=2000)

# Save
torch.save(model.state_dict(), 'protonet.pt')

"""## Evaluation"""

def evaluate(model, X_data, y_data, device='cuda', 
             k_shot=5, n_way=5, n_query=5, episodes=1000):
    '''
    Evaluate the ProtoNet on k-shot n-way task on the test set
    Args:
        model
        X_data (torch.Tensor): images
        y_data (torch.Tensor): labels
        k_shot (int): number of support images per class
        n_way (int): number of classes
        n_query (int): number of query images per class
        episodes (int): number of steps/episodes
    '''
    
    model.to(device)
    model.eval()
   
    test_losses = 0
    test_accuracies = 0

    for episode in tnrange(episodes, desc='Evaluating'):            
        # Extract sample
        sample = extract_sample(X_data, y_data, k_shot, n_way, n_query).to(device)

        # Forward Pass on ProtoNet
        loss, stats = model(sample, k_shot, n_way, n_query, device)

        # Get Stats
        test_losses += stats['loss']
        test_accuracies += stats['acc']

        if episode % 200 == 0:
            print('At Episode {:d} -- Loss {:.5f} - Acc: {:.3f}'.format(episode, stats['loss'], stats['acc']))

    # Print Epoch Stats
    mean_loss = test_losses / episodes
    mean_acc = test_accuracies / episodes
    print('Evaluation Results -- Loss: {:.5f} - Acc: {:.3f}\n'.format(mean_loss, mean_acc))

evaluate(model, X_eval, y_eval, device='cuda', k_shot=5, n_way=5, n_query=5, episodes=1000)

"""### Test on single sample"""

device = 'cuda'
k_shot = 5
n_way = 5
n_query = 5

# Sample a set from test set
my_sample = extract_sample(X_eval, y_eval, k_shot, n_way, n_query).to(device)
display_sample(my_sample.cpu())

# Predict
loss, stats = model(my_sample, k_shot, n_way, n_query, device)
stats

"""## Prediction on unlabelled Query Images
Making predictions on unlabelled query images given a new support set.
"""

from collections import Counter

def read_support_set(support_dir):
    '''
    Read a Support Set from a directory
    Args:
        support_dir (str): folder containing the support set with each sub-folder being a class.
                           Each class should have equal number of images i.e. consistent k_shot
    Returns:
        tuple of:
            (ndarray): X images
            (ndarray): y labels
            (dict) of: k_shot (int) and n_way (int)
            
    '''
    labels = os.listdir(support_dir)
    n_way = len(labels)
    
    X_support = []
    y_support = []
    
    for label in labels:
        label_path = os.path.join(support_dir, label)
        images = os.listdir(label_path)
        
        for img in images:
            image = cv2.imread(os.path.join(label_path, img))
            image = cv2.resize(image, (28, 28)) / 255
            
            X_support.append(image)
            y_support.append(label)
            
    k_shot = len(y_support) // n_way
            
    assert len(set(dict(Counter(y_support)).values())) == 1, 'Each Class should have equal number of images'
            
    return np.array(X_support), np.array(y_support), {'k_shot': k_shot, 'n_way': n_way}

def read_query_set(query_dir):
    '''
    Read a Query Set from a directory
    Args:
        query_dir (str): folder containing the unlabelled query images
    '''
    images = os.listdir(query_dir)
    
    X_query = []
    
    for img in images:
        image = cv2.imread(os.path.join(query_dir, img))
        image = cv2.resize(image, (28, 28)) / 255

        X_query.append(image)
            
    return np.array(X_query)

def predict(model, X_support, y_support, X_query, k_shot, n_way, n_query, device='cuda'):
    '''
    Make Predictions on new sets with unlabelled query images
    Args:
        X_support (ndarray): support set images
        y_support (ndarray): support set labels
        X_query (ndarray): unlabelled query images
        k_shot (int): number of images per class
        n_way (int): number of classes
        n_query (int): number of query images
    Returns:
        (tuple) of:
            (tensor): predictions
            (tensor): support samples
            (tensor): queries samples
    '''
    model.to(device)
    model.eval()
    
    # Convert support set images to sample format (5D)
    support = extract_sample(X_support, y_support, k_shot, n_way, n_query=0, permute=False).to(device)
    
    # Convert queries to tensor (4D)
    queries = torch.from_numpy(X_query).float().permute(0, 3, 1, 2).to(device)
    
    # Concatenate support set and query set along the 0th dimension
    x = torch.cat([support.contiguous().view(n_way * k_shot, *support.size()[2:]), 
                   queries.contiguous()], 0)
        
    # Compute embeddings
    embeds = model.encoder(x)
    z_dim = embeds.size(-1)
    
    # Compute Prototypes
    protos = embeds[:n_way * k_shot].view(n_way, k_shot, z_dim).mean(1)
        
    # Get query embeddings
    query_embeds = embeds[n_way * k_shot:]
    
    # Compute Distances
    distances = model.euclidean(query_embeds, protos)
    
    # Compute Softmax over the inverse of euclidean distances
    log_p = F.softmax(-distances, dim=1)
    
    # Compute Predictions
    y_hat = log_p.argmax(1)
    
    return y_hat, support, queries

X_support, y_support, task = read_support_set('prediction/support-2shot/')

X_query = read_query_set('prediction/query-2shot/')

# Make predictions
preds, support, queries = predict(model, X_support, y_support, X_query,
                                  k_shot=task['k_shot'], n_way=task['n_way'], n_query=len(X_query), 
                                  device='cuda')

print(preds)
# Display Query images
query_out = torchvision.utils.make_grid(queries.cpu(), nrow=len(queries))
plt.figure(figsize = (7,7))
plt.imshow(query_out.permute(1, 2, 0))
# Display Support images
display_sample(support.cpu())

